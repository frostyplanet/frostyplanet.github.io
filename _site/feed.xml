<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Warp 10</title>
    <description>&quot;Warp 10&quot; is impossible speed in StarTrek universe
</description>
    <link>http://blog.warp10.info/</link>
    <atom:link href="http://blog.warp10.info/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 10 Sep 2015 19:20:43 +0800</pubDate>
    <lastBuildDate>Thu, 10 Sep 2015 19:20:43 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;由于blogofile不维护很久，迁移到了github pages&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Sep 2015 02:15:19 +0800</pubDate>
        <link>http://blog.warp10.info/jekyll/update/2015/09/11/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/jekyll/update/2015/09/11/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>新年杂记</title>
        <description>&lt;p&gt;前两个月觉得自己似乎变成了工作狂，睡眠不足，焦虑，然而却觉得什么都不顺。反思一下其实是自己一直以来，为了急速前行而倾向于对一些细枝末节做减法。
走到后面发觉就是回过头来一无所有，不看新闻，不刷社交网络，不看书，不关心衣食住行，甚至懒得去学其他东西，连手机出很多问题都没工夫去折腾。
所以说工作狂是没有前途的，可以算得上是另一种逃避。&lt;/p&gt;

&lt;p&gt;回到家突然被告知外公已经离去了，本来两个月前外婆去世，就担心外公会接受不了，想趁新年回去看看，谁知道也晚了一步。
于是第二天回乡下，但因为过年的缘故，等了几天才办事，最后把他葬在外婆的旁边，一个偏远的地方。上次外婆走的时候没能赶回来，心里已经很是愧疚。撒下泥土，点过香烛之后，也给外婆上了一炷香。&lt;/p&gt;

&lt;p&gt;后来和表妹谈到是否有灵魂的问题。小时候一想到自己有一日也会不复存在，就会觉得很害怕，大人会说到了老的那个时候就会看开了。现在联想到一些曾经有过的第六感，还有 做过的奇怪的梦，我还是愿意相信有一些科学解释不了的事情。或许这能够给人更坦然的面对这些事情吧。&lt;/p&gt;

&lt;p&gt;最近也在看”和摩根弗里曼一起穿越虫洞”，有一集说到每个人本身就在做通往未来的时间旅行，眼睛所看到的光线，大脑接受的信息，一切都是已经发生的事情。
科幻作品中什么回到过去，无论是能改变历史还是不能改变历史，从逻辑上都不怎么可能。所以才说当下是多么的珍贵。&lt;/p&gt;

&lt;p&gt;有时候想想人是挺渺小的，和地理或者是天文上的时空概念相比。今天站在的荒无人烟的山坡上，n年后或许变成城镇，或许变成高山或大海。对比一个人的得或失，或者生活的顺或不顺，也不算什么了。&lt;/p&gt;

&lt;p&gt;以前我也是一个叛逆的小孩，面对父母唠叨感觉厌烦，想有那么远跑那么远，一直不想和70年代的人那样规矩地生活，但是也不能像90后那样无拘无束，有种身份认同障碍。现在也会意识到变成熟是多么的重要，总有一天要负担起照顾父母教育小孩的责任。&lt;/p&gt;

&lt;p&gt;“Emotional Equation”这本书里面讲，寻找生活的意义，在于“生活向我要求什么”，存在于每日每时。不是在于“通道尽头的光明”，不在于“言语和苦思”, ，更不要等到世界末日只剩几天才去寻求改变。&lt;/p&gt;

</description>
        <pubDate>Fri, 07 Feb 2014 20:00:00 +0800</pubDate>
        <link>http://blog.warp10.info/misc/2014/02/07/new-year.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/misc/2014/02/07/new-year.html</guid>
        
        
        <category>misc</category>
        
      </item>
    
      <item>
        <title>python-socket-engine v1.1  with generator coroutine support</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://github.com/frostyplanet/python-socket-engine&quot;&gt;项目页面&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;先说一下过往的历史。&lt;/p&gt;

&lt;p&gt;这其实是一个自娱自乐的项目。刚开始的时候只是想做一个纯python的兼容多线程的异步通信框架。虽然现在python下面选择很多，写web的有各种wscgi的实现，通用的也有很多。
像标准库里面的SocketServer(最多只能加上ThreadingMixIn/ForkingMixIn)；
asyncore没有超时，伪异步模型，没有定长的读写函数，在一个handler里面实现复杂的状态机也是个很头痛的事情；
twisted，thrift等好像都是要把逻辑拆成transport/dispatch/server, 从设计模式上觉得很复杂不灵活，另外也局限于同步模式的效率问题。
而gevent打monkey patch是很方便，替换了threading api和读写操作，不过想起来以前在perl下面用Coro/EV/AnyEvent的时候有诡异的各种疑难杂证，这也不例外。&lt;/p&gt;

&lt;p&gt;所以总体上没有一个合心水的。所以参照像POE那样的基于回调的样子，自己写一个，每个异步(connect/read/write/readline)的操作都有两个回调(ok/error)，自动对读/写/空闲连接做超时检查，出错的时候触发error回调并且自动close connection。
如果需要性能，那就把业务逻辑组织成串联起来的回调，如果需要短平快的实现，那就做线程池的同步实现，都十分灵活。
不过这也是写了很多服务端和客户端逐渐改进过来的，历时两年多了。&lt;/p&gt;

&lt;h2 id=&quot;change-log&quot;&gt;Change Log&lt;/h2&gt;

&lt;p&gt;v0.3 用在配管项目，那个时候每个connection都有一个状态，底层的poll实现只能注册一种事件，读写还不能是同时的，开始的时候异步方式要比同步方式慢一倍。&lt;/p&gt;

&lt;p&gt;v0.4 用在克隆项目，做了一个简单的异步http方式接口&lt;/p&gt;

&lt;p&gt;v0.7 加了ssl支持和rpc client/server&lt;/p&gt;

&lt;p&gt;v0.9 写了transwarp私有协议代理，改成每个connection读写可以不互相影响，网速快的时候可以跑出十多兆的流量吞吐，本地回环上的顺序读写测试可以达到千兆速率。异步回调方式已经可以和同步方式一样快了&lt;/p&gt;

&lt;p&gt;v1.0 加了readahead的特性，最大限度减少了异步方式额外开销。&lt;/p&gt;

&lt;p&gt;v1.1 加了coroutine支持。异步回调可以转变为在generator中yield的写法。&lt;/p&gt;

&lt;h2 id=&quot;api&quot;&gt;API约定&lt;/h2&gt;

&lt;p&gt;基本上每个server其实都有一个(或分阶段有多个) readable的handler，用来放主要的逻辑入口。新链接被listener accept之后，先经过new_conn_cb() 回调来根据自定义的逻辑过滤或握手，然后被put_sock（）放进poll里头等待i/o事件。&lt;/p&gt;

&lt;p&gt;在可读时候就触发回调readable_cb()。然后经过一定的read和write操作，当要暂停处理的时候调用remove_conn（）从事件引擎中移除 (比如转交给线程池的worker做阻塞方式读写什么的) 。或者处理完毕再watch_conn() 把链接放进poll中等待readable事件。在watch_conn之后connection会变成idle状态，如果链接是长链接的话，也可以通过idle timeout超时来移除长期不活动链接。&lt;/p&gt;

&lt;p&gt;如果是客户端方式的connection的话，里面相应的readable回调为空，读写操作和服务端一样。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;性能优化思路&lt;/h2&gt;

&lt;p&gt;对于python来说，每一次函数调用的开销都是可见的，系统调用的开销更大（因为异步模式需要register/unregister 事件和回调）。
所以为什么一开始的时候，异步回调的server会比同步模式的server要用额外的2/3到一倍的时间。越是内层的事件循环，每个语句的开销更加明显，有时候频繁访问的对象属性多一层引用，或者多几个额外的赋值，都会有影响。&lt;/p&gt;

&lt;p&gt;往poll中register fd的开销是最明显的。所以每次read_unblock()/write_unblock()都要先尝试读/写，遇到EAGAIN之后才往poll中注册。还有注册fd的时候先看看自己维护的字典，如果注册的事件一样，那就只改回调函数，省去了重新注册fd。&lt;/p&gt;

&lt;p&gt;对于已经完成的读操作，由于框架不知道后面是不是还要读，所以并不会自作主张的把已经注册的读事件注销，最初要使用者显式调用remove_conn()，但如果不注意的漏写了的话，在一些度操作之后可能由于写操作的时间比较长，先前注册的读回调被意外触发，这当然会引发诡异的程序行为。所以在v1.0版本中，通过在读操作完成之后，把先前注册的读回调替换成readahead的回调，readahead缓冲如果满了再注销。这样下次读的时候就直接从缓冲中取就行了。一举两得，上层既不用去管下层的事件register/unregister，同时也节省了一些系统调用的开销。而写操作，如果往poll中注册过事件的话，操作完了当然是要立刻注销的。&lt;/p&gt;

&lt;p&gt;对于EPoll的edge-trigger特性，也会有问题需要避免。比如说system buffer中有50k数据可用，一次事件触发的时候由于业务逻辑的需要只读了20k，如果没有unregister fd的话，下一次watch_conn()之后可读事件并不会触发，所以在实现read_unblock的时候，总是比要求的长度去多读一点，多出来的放在readahead buffer中，下次watch的时候如果readahead buffer有数据的话，可以跳过register fd的步骤，立刻调用readable_cb() ，如果没有数据的话，证明上一次的已经读完了，这时候register fd是安全的。&lt;/p&gt;

&lt;p&gt;关于回调的效率，由于有时候读写可以跳过poll的注册直接完成，直接执行的比间接回调要快，但是直接执行回调会容易出现递归过多超过了栈大小的问题，所以给每个connection对象保存了递归调用的计数，超过的时候就要把回调放进
一个pending的队列中等下一个poll循环触发。&lt;/p&gt;

&lt;p&gt;暂时就想到这么多了。&lt;/p&gt;

&lt;h2 id=&quot;evevent&quot;&gt;关于EV、event等事件引擎&lt;/h2&gt;

&lt;p&gt;由于之前都是用select.poll和select.epoll来实现事件循环，试了一下pyev，不像通常认为的C实现的要比python快，反而比起先前的我用select.epoll写的模块没有性能优势，可能由于里面由于复杂的各种逻辑(timer/process watcher等)额外的开销, 另外EV为了安全起见使用的EPoll后端是level-trigger的，好像没有改成edge-trigger的选项。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;写了这么久终于轮到协程了&lt;/h2&gt;

&lt;p&gt;回调方式的异步的问题就是程序写起来不直观，如果有多个读/写操作的话，每一步都要拆成一个回调。有时候函数的参数也容易传错。更重要的是，抛异常的时候，如果刚好是被多个地方用到的共同的回调，
看堆栈信息很难找到出问题来源。&lt;/p&gt;

&lt;p&gt;比如说看下面这样一个测试代码片段，是不是眼花了? 写过一次的代码都不想再写第二遍。而且回调定义的顺序需要和执行顺序倒过来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def test_connect (self):
    event = threading.Event ()
    err = None
    def __on_err (conn):
        print conn.error
        self.fail (conn.error)
    def __on_conn_err (e):
        err = &quot;conn&quot; + str(conn.error)
        event.set ()
    def __on_read (conn):
        print &quot;read&quot;
        self.client.engine.close_conn (conn)
        buf = conn.get_readbuf ()
        if buf == self.data:
            print &quot;ok&quot;
            pass
        else:
            err = &quot;test connect error, invalid data received: %s&quot; % (buf)
        event.set ()
    def __on_write (conn):
        print &quot;write&quot;
        self.client.engine.read_unblock (conn, len (self.data), __on_read, __on_err)
        return
    def __on_conn (sock):
        print &quot;on connect&quot;
        self.client.engine.write_unblock (Connection (sock), self.data, __on_write, __on_err)
        return
    self.client.engine.connect_unblock (self.server_addr, __on_conn, __on_conn_err)
    print &quot;connect unblock&quot;
    event.wait ()
    if not err:
        print &quot;* test connect ok&quot;
    else:
        self.fail (err)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而coroutine就是把同步过程异步化的最好的方式。但是我并不想用gevent或者evenlet这些侵入式的实现，如果有纯python的话就最好了。前天找了一下终于看到了&lt;a href=&quot;git://github.com/sampsyo/bluelet.git&quot;&gt;bluelet&lt;/a&gt;这个东西。众所周知，由于python generator只能保存一层堆栈，所以bluelet里面做了一个专门的事件循环来跟踪不同coro的关系。不过现有版本额外开销有点大，所以拿过来重写了一下，顺便和我的框架整合。
纯python实现的好处就是，多开若干真实的线程，把接受到的链接round robin到不同的线程中poll，或者用异步方式接收请求，把会阻塞的工作放到线程池中处理，可以最大限度的利用cpu。&lt;/p&gt;

&lt;p&gt;对照上面的代码片段，coro方式的如下，应该看起来直观很多了。唯一要注意的是不能忘了yield。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def test_connect (self):
    event = threading.Event ()
    err = None
    def _client ():
        _data = None
        try:
            engine = self.client.engine
            print &quot;connecting&quot;
            conn = yield engine.connect_coro (self.server_addr)
            print &quot;connected&quot;
            yield conn.write (self.data)
            _data = yield conn.read (len (self.data))
            conn.close ()
        except Exception, e:
            getLogger (&quot;client&quot;).exception (e)
            self.fail (e)
            return
        if _data == self.data:
            print (&quot;conn &amp;amp; read ok&quot;)
            event.set ()
        else:
            err = &quot;test connect error, invalid data received: %s&quot; % (buf)
            event.set ()
            self.fail (err)
        return
    self.client.engine.run_coro (_client)
    event.wait ()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;coroutine&quot;&gt;coroutine模式性能测试&lt;/h2&gt;

&lt;p&gt;环境intel i3-2310M (关了超线程)&lt;/p&gt;

&lt;p&gt;阻塞方式4线程客户端，每个线程顺序读写100k数据并且循环50000次，测试从开始到结束的时间，多次测试取最快的一次，在本地回环接口测试。&lt;/p&gt;

&lt;p&gt;服务端规定要每次接收到100k数据后才能完整写回。&lt;/p&gt;

&lt;p&gt;异步回调方式的服务端 : 18.549s (v0.9), 17.954s (v1)&lt;/p&gt;

&lt;p&gt;单线程阻塞方式的伪并发服务端: 19.98s (v0.9), 18.68s (v1)&lt;/p&gt;

&lt;p&gt;纯python协程的服务端:  优化前（基于bluelet的原有代码） 31.154s,   优化后(v1.1) 21.735s&lt;/p&gt;

&lt;p&gt;所以还是有一些额外开销，暂时比较满意了。&lt;/p&gt;

</description>
        <pubDate>Thu, 30 May 2013 22:33:32 +0800</pubDate>
        <link>http://blog.warp10.info/coding/2013/05/30/socket-coro.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/coding/2013/05/30/socket-coro.html</guid>
        
        
        <category>coding</category>
        
      </item>
    
      <item>
        <title>文件系统知识入门 (reiserfs3.6)</title>
        <description>&lt;p&gt;昨晚睡觉之前看了一下&lt;a href=&quot;http://lwn.net/Articles/342892/&quot;&gt;a short history of brtfs&lt;/a&gt;写得非常好的一篇文章，有种眼前一亮认知被刷新了的感觉。&lt;/p&gt;

&lt;p&gt;如果说ext是来自过去，brtfs/ceph是未来，reiserfs的设计还是足够现代的 (忽略政治因素和个人因素)，关于文件系统树，关于packing small objects into a block这些理念都能在brtfs里面找到。所以我还是觉得应该回头重新了解一下reiserfs3.6，虽然namesys的官网都已经不存在了，and reiser4 will probably never make it to the mainline。&lt;/p&gt;

&lt;p&gt;说到底，数据库设计有CAP的矛盾，文件系统里面也有各种矛盾：locality，碎片和元素据等空间利用率，查找效率，小文件效率，大文件吞吐…所以应该带着这些问题来看。&lt;/p&gt;

&lt;p&gt;——- 不太自然的分割线———-&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;层次&lt;/h2&gt;
&lt;p&gt;在ext文件系统中，有block group，inode这些比较固定的层次。inode都只能是block aligned的，所以存在利用率不足、inode用光了这些尴尬的问题。目录索引、日志这些都是后来才加上去的，也有挺别扭的各种特性开关。
reiserfs里面能看到除了超级块、用于标记空闲的bitmap块、日志块之外，其他所有的东西都作为对象放在b树里头。(其实reiserfs称之为s+tree, 在内核代码里面有stree.c)
而brtfs更进一步，所有东西都在cow的b树里头。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;文件系统树&lt;/h2&gt;
&lt;p&gt;block按照在b树中的层次分internal node和作为叶子的formated node，都是以一个块为长度。其实要较真的话的确比普通b+树有些变化。&lt;/p&gt;

&lt;p&gt;&lt;b&gt;internal node的块&lt;/b&gt;是一个标准的b树节点的样子, 其中的pointer指向下层的块。&lt;/p&gt;
&lt;table&gt;
&lt;tr bgcolor=&quot;#FFCCCC&quot;&gt;
&lt;td bgcolor=&quot;#99FFCC&quot;&gt;&amp;nbsp;Block_Head&amp;nbsp;&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCCFF&quot;&gt;&amp;nbsp;Key 0&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCCFF&quot;&gt;&amp;nbsp;Key 1&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCCFF&quot;&gt;---&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCCFF&quot;&gt;&amp;nbsp;Key N&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCCCC&quot;&gt;&amp;nbsp;Pointer 0&lt;/td&gt;
&lt;td&gt;&amp;nbsp;Pointer 1&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;td&gt;&amp;nbsp;Pointer N&lt;/td&gt;
&lt;td&gt;&amp;nbsp; Pointer N+1&lt;/td&gt;
&lt;td bgcolor=&quot;#FFFFCC&quot;&gt;....Free Space....&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;b&gt;formated node是树的叶子节点。&lt;/b&gt;&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td bgcolor=&quot;#99FFCC&quot;&gt;&amp;nbsp;Block_Head&amp;nbsp;&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCC99&quot;&gt;&amp;nbsp;IHead 0&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCC99&quot;&gt;&amp;nbsp;IHead 1&amp;nbsp;&lt;/td&gt;
&lt;td bgcolor=&quot;#FFCC99&quot;&gt;&amp;nbsp;IHead 2&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;td&gt;&amp;nbsp;IHead N&lt;/td&gt;
&lt;td bgcolor=&quot;#FFFFCC&quot;&gt;....Free Space....&lt;/td&gt;
&lt;td&gt;&amp;nbsp;Item N&amp;nbsp;&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;td&gt;&amp;nbsp;Item 2&lt;/td&gt;
&lt;td&gt;&amp;nbsp;Item 1&lt;/td&gt;
&lt;td&gt;&amp;nbsp;Item 0&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;在这样一个formated node里面可以pack很多个item，item是排序的，可以看到head和body是分开放在block的两头，顺序相反向中间增长。head里面有指针到后面的body。
struct block_head里面的blk_right_delim_key就是作为叶子节点的key，应该就是这个block里面最后一个item的key (我猜的)，不过从其他文档说
已经不用了(only kept for compatibility), 其实也可以根据节点里面最后一个item得知。&lt;/p&gt;

&lt;p&gt;所有formated node的路径(path)高度都是相同的。而unformated node(用来放大文件的数据)是挂在formated node中的indirect item下面，所以path的长度+1。&lt;/p&gt;

&lt;h2 id=&quot;item&quot;&gt;item分类&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;direct tems  :  tails of files&lt;/li&gt;
  &lt;li&gt;indirect items  N * (point to unformated node) (file blob)&lt;/li&gt;
  &lt;li&gt;directory items :  deHead * N + filename * N 
 N = item_head.ih_entry_count&lt;/li&gt;
  &lt;li&gt;stat item: 文件元数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Files consists of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;StatData item + [Direct item]                      (for small files : size from 0 bytes to MAX_DIRECT_ITEM_LEN=blocksize-112 bytes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;StatData item + multiple InDirect item  + [Direct item]      (for big files     : size    &amp;gt; MAX_DIRECT_ITEM_LEN bytes)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Directory consists of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;StatData item + multiple Directory item&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key&quot;&gt;Key&lt;/h2&gt;
&lt;p&gt;这里贴的是version2的格式&lt;/p&gt;
&lt;table border=&quot;1&quot;&gt;
&lt;thead&gt;
&lt;tr&gt;
	&lt;th&gt; Name &lt;/th&gt;
	&lt;th&gt; Size &lt;/th&gt;
	&lt;th&gt; Description &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
	&lt;td&gt; Directory ID &lt;/td&gt;
	&lt;td align=&quot;center&quot;&gt;  4 &lt;/td&gt;
	&lt;td&gt;  the identifier of the directory where the object is located &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt; Object ID &lt;/td&gt;
	&lt;td align=&quot;center&quot;&gt;  4 &lt;/td&gt;
	&lt;td&gt;  the actual identifier of the object (&quot;inode number&quot;) &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt; Offset &lt;/td&gt;
	&lt;td align=&quot;center&quot;&gt;  60 bits &lt;/td&gt;
	&lt;td&gt;  the offset in bytes that this key references 一个文件或目录用多个item来表示，offset会递增 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
	&lt;td&gt; Type &lt;/td&gt;
	&lt;td align=&quot;center&quot;&gt;  4 bits &lt;/td&gt;
	&lt;td&gt;  the type of item. Possible values are:&lt;br /&gt;Stat: 0&lt;br /&gt;Indirect: 1&lt;br /&gt;Direct: 2&lt;br /&gt;Directory: 3&lt;br /&gt;Any: 15 &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Only stat items have an offset of 0. Files (direct and indirect items) and directories always start with an offset of 1 so that they are sorted behind the stat item in the leaf nodes. For directory items the “offset” field contains the hash value and generation number of the leftmost directory header of the directory item (see below), not the offset in bytes.&lt;/p&gt;

&lt;p&gt;可以看到，根据b树的特性，同一个目录下面的文件和目录会在索引中邻近，并且会按照在父目录中的位置排序。这里面object id指的就是unix中的inode编号。&lt;/p&gt;

&lt;p&gt;Node Layout和tree ordering是值得好好看的一部分
[有时间再待续]&lt;/p&gt;

</description>
        <pubDate>Sat, 18 May 2013 18:30:00 +0800</pubDate>
        <link>http://blog.warp10.info/fs/storage/2013/05/18/filesystem2.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/fs/storage/2013/05/18/filesystem2.html</guid>
        
        
        <category>fs</category>
        
        <category>storage</category>
        
      </item>
    
      <item>
        <title>文件系统知识入门</title>
        <description>&lt;p&gt;不知道为什么会转头看文件系统，大概因为以前曾经试图看过reiserfs看不出个所以然来，完成一下心愿。因为一天之内看得太多，需要记一下，组织得有些乱。&lt;/p&gt;

&lt;h2 id=&quot;directory-index&quot;&gt;Directory Index&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://ext2.sourceforge.net/2005-ols/paper-html/node3.html&quot;&gt;link1&lt;/a&gt;
&lt;a href=&quot;http://static.usenix.org/publications/library/proceedings/als01/full_papers/phillips/phillips_html/index.html&quot;&gt;link2&lt;/a&gt;
早期的时候ext2为了实现简单，一个目录结构里面（文件/子目录）是挨个放在一起只能线性搜索的，当一个目录里面文件很多，缺点显而易见。所以开发者给打了一个patch，实现了一个类似B+树的Htree索引结构，
一直被ext2/3/4沿用下来。
key是文件名的hash，合并/分裂都和B+tree或者B*tree类似，叶子节点是正常的目录项，只不过存储的时候不排序，当分裂/合并的时候才需要重新排序。&lt;/p&gt;

&lt;p&gt;相比之下，Reiserfs是B*树，也是使用固定长度的hash做键值。而JFS，是使用变长健的prefix B-tree,叶子节点是full-length。定长的hash key的好处也很显然，一是内部节点可以放很多个key，可以做到高扇出（因此树的高度可以更小）;
二是方便key对齐，键值拷贝的时候和查找都比较方便。&lt;/p&gt;

&lt;p&gt;觉得要支持变长健的b树实现起来其实挺麻烦，除了prefix B-tree之外，参考论文”Pagination of B*-trees with variable length records”, “efficient optimal pagination of scrolls”，还有&lt;a href=&quot;http://people.apache.org/~shv/STrees.html&quot;&gt;S(b)tree&lt;/a&gt;，一种目的是提高满度(存储更紧凑)的generialized B-tree。&lt;/p&gt;

&lt;h2 id=&quot;the-problem-of-inode-limit-on-ext234&quot;&gt;The problem of inode limit on ext2/3/4&lt;/h2&gt;
&lt;p&gt;以前会遇到一个ext2文件系统inode用光了的情况，这是因为在mkfs的时候inode就预先分配了，可以看到&lt;a href=&quot;https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#Layout&quot;&gt;ext4 disk layout&lt;/a&gt;里头，有专门的inode bitmap和inode table，superblock里头s_blocks_per_group, s_inodes_per_group都是静态的。不像reiserfs、btrfs是可以动态分配的。&lt;/p&gt;

&lt;h2 id=&quot;file-block-mapping-direct-block-indirect-block&quot;&gt;File-Block Mapping: Direct block, Indirect block&lt;/h2&gt;
&lt;p&gt;传统的单层/二层/三层块映射表，在教科书里已经说得很多了，每个表项只能映射一个block。缺点就是如果有很大的连续空间，在映射表里头还是要老老实实地添加每个映射项，个数多的时候读取起来也慢。
所以为什么新的设计ext4、brtfs改成了extent tree的映射方式。&lt;/p&gt;

&lt;h2 id=&quot;file-block-mapping-extent&quot;&gt;File-Block Mapping: Extent&lt;/h2&gt;
&lt;p&gt;有一系列图文并茂的文章讲到ext4上面的extent。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://computer-forensics.sans.org/blog/2010/12/20/digital-forensics-understanding-ext4-part-1-extents&quot;&gt;digital forensics understanding ext4: part1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://computer-forensics.sans.org/blog/2011/03/28/digital-forensics-understanding-ext4-part-3-extent-trees&quot;&gt;digital forensics understanding ext4: part3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://computer-forensics.sans.org/blog/2011/04/08/understanding-ext4-part-4-demolition-derby&quot;&gt;digital forensics understanding ext4: part4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;也可以参考&lt;a href=&quot;https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout#The_Contents_of_inode.i_block&quot;&gt;具体的数据结构&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;struct ext4_inode.i_flags  如果有标志位EXT4_INODE_EXTENTS就表示用extent来映射，否则就是传统的direct/indirect mapping。&lt;/p&gt;

&lt;p&gt;ext4_inode.i_block(60字节)，由一个ext4_extent_header打头，根据其中的depth是否为0，决定后面的是最多四个的ext4_extent (extent树的叶子节点)，还是作为根的ext4_extent_idx(中间节点)。&lt;/p&gt;

&lt;p&gt;其中叶子节点的ee_block表示数据的逻辑块号，ee_start_hi&amp;amp;lo表示映射的物理块号，由于ee_len是16位的，一个extent就可以映射2^16个块，比direct/Indirect mapping要节省很多用于元数据的空间。
。而中间节点的ei_leaf_hi/lo表示下面子节点的物理块号。&lt;/p&gt;

&lt;h2 id=&quot;ext4block-allocate-policy&quot;&gt;ext4的block allocate policy&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;allocate 8k to the file first,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;if initial 8k used up,  delay commit until timeout/ sync() / kernel out of mem&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;try to keep a file in the same block group as the directory&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;optimise locality of directory near root&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可惜没有文章讲的更详细，ext4为了保持向前兼容性，还是被一些过时的设计所限制，回头还是应该去看看brtfs是怎么做的。&lt;/p&gt;

</description>
        <pubDate>Fri, 17 May 2013 23:20:00 +0800</pubDate>
        <link>http://blog.warp10.info/fs/storage/2013/05/17/filesystem.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/fs/storage/2013/05/17/filesystem.html</guid>
        
        
        <category>fs</category>
        
        <category>storage</category>
        
      </item>
    
      <item>
        <title>vector clock and eventual consistency</title>
        <description>&lt;p&gt;学习了一下Amazon Dynamo的文章。因为分布式系统CAP原理的限制，他们选择了损失一致性的前提来把可用性做到最大，强调的是always writable，还有根据业务动态调整Quorum NRW参数，可以想象为什么当时会引起很大的轰动。&lt;/p&gt;

&lt;p&gt;既然是最终一致性，也就只要并发存在，数据肯定存在不一致的时候。出现多个版本的数据的情况至少有两种：
1. 不同的客户端并发读写, 2. 在最新的数据还没有传递到相应副本节点的时候，这个节点被选择成为协调写操作的节点。&lt;/p&gt;

&lt;p&gt;跟踪多个版本，用到了vector clock。(Vector Clock的研究有很多，可以看&lt;a href=&quot;http://net.pku.edu.cn/~course/cs501/2008/reading/a_tour_vc.html&quot;&gt;A Practical Tour of Vector Clock Systems&lt;/a&gt;这篇survey)。不过需要注意的是和&lt;a href=&quot;http://en.wikipedia.org/wiki/Vector_clock&quot;&gt;wikipedia&lt;/a&gt;上面说的并不太一样。数据库只在写入的时候增加vector clock, 一个value的数据副本发送到其他节点的时候，附带的vector clock是不会增加的。一个节点上收到多个版本的数据的时候，如果任意两个版本中存在偏序，小的那个可以被丢弃；如果判断不了，就交给客户端在读取的时候去由业务逻辑做合并。&lt;/p&gt;

&lt;p&gt;比如说，下面的偏序关系是成立的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A1 &amp;lt; A1B1,  B1 &amp;lt; A1B1,  A1B1 &amp;lt; A1B2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是当有A1B1和A1C1的时候，就意味着存在冲突了，由客户端逻辑合并之后的版本就是A1B1C1，覆盖掉之前的A1B1和A1C1。&lt;/p&gt;

&lt;p&gt;如果把vector clock中的版本方式换成一定精度的时间戳，也是可以的，而且让解决冲突的时候比较方便一些，当vector clock变得过长需要截断的时候也方便一些。&lt;/p&gt;

&lt;p&gt;另一个基于dynamo思想的k-v存储Riak，他们有两篇vector clock的文章也说的比较显浅&lt;a href=&quot;http://basho.com/why-vector-clocks-are-easy/&quot;&gt;why-vector-clocks-are-easy&lt;/a&gt;，&lt;a href=&quot;http://basho.com/why-vector-clocks-are-hard/&quot;&gt;why-vector-clocks-are-hard&lt;/a&gt;。只不过他们的不同之处是将vector clock暴露给客户端，用client-id去作向量的维度，区别于dynamo论文说的用coordinator的node-id作维度。不过我觉得这种方法在把复杂性暴露给客户端，还有在client很多的情况下让vector clock变得很大这些缺点，优点其实可以忽略掉。
我觉得既然放弃了事务特性而选择了最终一致，那就意味着业务逻辑必须要接受一致性损失。如果非要纠结的话，那还不如读写串行化或者改用其他支持事务的实现来得更实际。&lt;/p&gt;

</description>
        <pubDate>Tue, 07 May 2013 19:52:00 +0800</pubDate>
        <link>http://blog.warp10.info/distributed/db/storage/2013/05/07/vector-clock.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/distributed/db/storage/2013/05/07/vector-clock.html</guid>
        
        
        <category>distributed</category>
        
        <category>db</category>
        
        <category>storage</category>
        
      </item>
    
      <item>
        <title>Paxos实践</title>
        <description>&lt;p&gt;有一句话叫做，”世上只有一种分布式一致性协议，那就是Paxos“，之前所有协议（如2步提交等）的思想都可以被paxos所包含了。
于是这两天在看了几篇paper，先是Lamport的”Paxos Made Simple”，但是没有去看”the Part-time Parliament”，Lamport自己也说了，先前那篇论文对很多人来说像希腊文一样难以理解-_-，大家都不懂欣赏他的幽默…
“Paxos Made Simple”这篇中已经描述了基本的Paxos协议，和探讨把多个事务(paxos实例)串联起来，不过最后关于银行系统实现的章节举例的比较简略。
后来又去看出自google chubby分布式锁项目的那篇”Paxos Made Live”，说到MultiPaxos这种演化出来的协议怎么从工程角度提高paxos的性能和避免一系列的复杂容错问题。另外英文维基上也讲了各种像Fast Paxos, 
Cheap Paxos，Generalized Paxos等演化版本。&lt;/p&gt;

&lt;p&gt;关于paxos的几个前提限定的文字在维基上都说的很简单，然而光看了还是对具体的算法步骤不太清楚，过的时间越久好像变得越糊涂，所以开始写一个python的演示代码来看看具体怎么运行。repo在&lt;a href=&quot;https://github.com/frostyplanet/paxos&quot;&gt;这里&lt;/a&gt;。写的过程中不得不又仔细推敲了paxos那几个限定条件，实际上BasicPaxos的实现也不需要很多代码，然后很多疑惑都烟消云散了。&lt;/p&gt;

&lt;p&gt;目前暂时只完成了多个实例在多线程下的模拟（包括proposer互相竞争的过程），后面todo的还有节点失效的模拟，网络协议的整合，有时间的话还打算做一下multipaxos和关于动态成员管理的功能。&lt;/p&gt;

&lt;p&gt;另外还有关于实现的两篇论文可以参考，”Paxos Made Practice”和”Paxos Made Code”。&lt;/p&gt;

&lt;p&gt;ps，记录一下我自己理解到的要点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;同一个paxos实例可以被多次运行，要决定的value可以是任何东西。如果单纯用paxos来选举leader的话，随机给一个值就行了。当然也可以把一些像事务日志当成value分发给众replica&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;保证promise的和accepted的proposer的序号的全局有序性。acceptor角色，prepare阶段，已经promise过的序号只能被更大的来覆盖。
如果一旦到了accept(决定)阶段，已经决定的值就达成一致不能再变更了，可以把这个看成是用paxos来进行的对众replica的写操作。
至于读操作，也可以通过发起一次完整paxos的，收到promise/accepted/nack会得知已经决定过的值。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 01 May 2013 23:00:00 +0800</pubDate>
        <link>http://blog.warp10.info/coding/distributed/2013/05/01/paxos.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/coding/distributed/2013/05/01/paxos.html</guid>
        
        
        <category>coding</category>
        
        <category>distributed</category>
        
      </item>
    
      <item>
        <title>transwarp project</title>
        <description>&lt;p&gt;最近很多vps的ssh和pptp都被封掉，包括linode日本和美国的一些地方，不知道是因为某软件的协议和ssh很像，还是因为他们把ssh代理的方式都要x掉。
在日本的一台vps和另一台美国的vps研究了一下，发觉被针对的不单单是针对特定协议。任何端口的tcp三次握手都很难达成，icmp包也周期性的不可达(正常的时候延时并不大)，估计是已经进化到路由表污染的方式。
虽然三层路由封锁我们没有办法，不过还是应该减少特定代理方式的使用以免触发什么规则。&lt;/p&gt;

&lt;p&gt;所以这几天写了个简单的东西，设想很简单，做一个sock5代理(可以运行在本地没人知道)，用私有协议把请求加密运送到远端出口，协议特征尽量少，异步方式支持大量链接。由于异步调用比较难写，观察到现在好像没有明显的bug。
repo在&lt;a href=&quot;https://github.com/frostyplanet/transwarp&quot;&gt;这里&lt;/a&gt;。换了一个vps之后部署起来，看youtube还算流畅。&lt;/p&gt;

&lt;p&gt;另外要让ssh登陆用上sock5代理的方式，需要装net-analyzer/openbsd-netcat，然后在 ~/.ssh/config中配置 (其中ip和端口就是你的代理地址):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ProxyCommand /usr/bin/nc.openbsd -X 5 -x 127.0.0.1:9000 %h %p
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 28 Mar 2013 13:30:00 +0800</pubDate>
        <link>http://blog.warp10.info/coding/2013/03/28/transwarp.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/coding/2013/03/28/transwarp.html</guid>
        
        
        <category>coding</category>
        
      </item>
    
      <item>
        <title>闭关十日感悟</title>
        <description>&lt;p&gt;昨天又看了一下马雁散文集，谈到庚信，引用了几句话，”路尘犹向水，征帆独背关”, “阵云千里散，黄河一代清”，觉得很有力量。&lt;/p&gt;

</description>
        <pubDate>Fri, 15 Mar 2013 14:10:00 +0800</pubDate>
        <link>http://blog.warp10.info/life/2013/03/15/safty.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/life/2013/03/15/safty.html</guid>
        
        
        <category>life</category>
        
      </item>
    
      <item>
        <title>写了个python的rpc实现来替代thrift</title>
        <description>&lt;p&gt;最近线上用apache Thrift实现的服务端变得越来越不靠普，现象就是用TServer.TThreadedServer做的服务端accept之后，ssl handshake的时候一直出错，让所有客户端都无法链接。重启也不一定能消除。线上用了大半年，原来每个月挂一两次，现在变成几个小时就挂。
抛的异常像这样，但是一直找不到答案：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Errno 8] _ssl.c:504: EOF occurred in violation of protocol
Traceback (most recent call last):
  File &quot;**********/server.py&quot;, line 48, in accept
	server_side=True, ssl_version=ssl.PROTOCOL_TLSv1)
  File &quot;/usr/lib64/python2.7/ssl.py&quot;, line 381, in wrap_socket
	ciphers=ciphers)
  File &quot;/usr/lib64/python2.7/ssl.py&quot;, line 143, in __init__
	self.do_handshake()
  File &quot;/usr/lib64/python2.7/ssl.py&quot;, line 305, in do_handshake
	self._sslobj.do_handshake()
SSLError: [Errno 8] _ssl.c:504: EOF occurred in violation of protocol
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;python的ssl模块底层是用的openssl库，_sslobj的方法都在c binding里头, 所以上面说的那个异常要调试实在比较困难。
其实我们这个服务每分钟的请求数只有500个左右，也不算多…&lt;/p&gt;

&lt;p&gt;因为实在忍受不了随时要重启服务。另外由于对thrift不爽其实由来已久，协议很复杂，文档不完善，server端也写得不咋地 (TSimpleServer性能不能指望，TThreadPoolServer除了用SIGKILL以外的信号都不能kill掉，TNonblockingServer没能跑起来好像也不支持SSL)。
看上去很新颖的东西未必符合自己需要，不得不自己写一个替代品。&lt;/p&gt;

&lt;p&gt;首先花了一日来研究ssl握手怎么在异步模式下工作。关键就是ssl.wrap_socket之后有一个do_handshake的步骤，
可以在wrap的时候通过参数让他不自动调用，然后在你的异步代码中把handshake显式整合进去。另外就是异步读写的时候，由于ssl socket是包装过的，i/o事件处理的时候需要捕捉SSL_ERROR_WANT_READ和SSL_ERROR_WANT_WRITE , 相当于普通socket的EAGAIN。&lt;/p&gt;

&lt;p&gt;在我之前的写框架里面搞定异步ssl之后，做rpc的server其实就比较简单，可以用内省把函数加进服务端handle。客户端的封装还是费了一点心思。相比thrift需要预定义结构和接口生成代码, 自己做这些当然没什么必要，改成用pickle来序列化request和response。然而由于对象不方便序列化，调用的参数和返回值只能用普通的内置类型来做结构体，如果结构体层次复杂的话其实挺麻烦的，因为我不想写太多代码检查结构和捕捉异常，于是想到参考BeautilfulSoup的方式，写了一个结构体的wrapper，重载一些操作符，可以用比较懒人的方式访问：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d = {&#39;a&#39;: 1, &#39;b&#39;:2}
e = AttrWrapper (d)
print &quot;e.name.name:&quot;, e.name.name 	# empty string
print &quot;e.a:&quot;, e.a, &quot;e.b:&quot;, e.b   	# 1 2
print &quot;e:&quot;, e  						# {&#39;a&#39;:1, &#39;b&#39;:2}  #但e是封装过后的对象
print &quot;e:&quot;, e.unwrap ().items()     # [(&#39;a&#39;, 1), (&#39;b&#39;, 2)]  #unwrap之后获得原本的dict
l = [1, 2, d]
e2 = AttrWrapper (l)
print &quot;e2:&quot;, e2  					# [1, 2, {&#39;a&#39;: 1, &#39;b&#39;: 2}]
print &quot;e2[0]:&quot;, e2[0]  				# 1
#print e2[3] 						# IndexError
print &quot;e2[2]:&quot;, e2[2]				# 获得上面和d一样的封装过后的对象
print &quot;b:&quot;, e2[2].b 				# 2
e_empty = AttrWrapper ({})
print &quot;e_empty:&quot;, e_empty  			# empty string
assert not e_empty
assert not e_empty.a
#print e2.a   						# KeyError
#print e[0]   						# IndexError
#print e.a.b  						# KeyError
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后搞定之后花了一天来移植业务逻辑，没日没夜地搞了两天，上线之后看着各种日志在刷屏，有种淡淡的空虚。&lt;/p&gt;

&lt;p&gt;这个框架的代码在&lt;a href=&quot;https://github.com/frostyplanet/python-socket-engine&quot;&gt;github python-socket-engine&lt;/a&gt;, 见test/test_rpc.py&lt;/p&gt;

&lt;p&gt;ps:  回头想了想，发现上面 print “e.name.name” 不抛异常是个错误的feature，因为这里的情景并不等同于访问层次很深的dom树，事实上e.name就应该返回None&lt;/p&gt;

</description>
        <pubDate>Wed, 13 Mar 2013 22:30:00 +0800</pubDate>
        <link>http://blog.warp10.info/coding/2013/03/13/rpc-implementation.html</link>
        <guid isPermaLink="true">http://blog.warp10.info/coding/2013/03/13/rpc-implementation.html</guid>
        
        
        <category>coding</category>
        
      </item>
    
  </channel>
</rss>
